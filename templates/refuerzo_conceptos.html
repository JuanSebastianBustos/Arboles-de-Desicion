{% extends "base.html" %}

{% block title %}Conceptos Básicos - Aprendizaje por Refuerzo{% endblock %}

{% block content %}
<div class="container mt-5 mb-5">
    
    <header class="text-center mb-5">
        <h1 class="display-4">Aprendizaje por Refuerzo</h1>
        <p class="lead text-muted">Teoría completa, Algoritmos y Buenas Prácticas</p>
    </header>

    <div class="card shadow-sm mb-5">
        <div class="card-header bg-primary text-white">
            <h2 class="h4 mb-0">1. Definición General</h2>
        </div>
        <div class="card-body">
            <p>
                El aprendizaje por refuerzo es una disciplina distinta donde no hay un supervisor que indique la acción correcta; 
                el agente recibe una señal (recompensa) después de actuar. Las consecuencias de las acciones 
                pueden ser retrasadas en el tiempo.
            </p>
            <div class="row mt-3">
                <div class="col-md-6 border-end">
                    <h6>Vs. Supervisado</h6>
                    <small>No hay un "profesor". El feedback es una recompensa, no una etiqueta correcta.</small>
                </div>
                <div class="col-md-6">
                    <h6>Vs. No Supervisado</h6>
                    <small>Se busca maximizar una señal de recompensa, no solo encontrar patrones ocultos en datos.</small>
                </div>
            </div>
        </div>
    </div>

    <div class="card shadow-sm mb-5">
        <div class="card-header bg-success text-white">
            <h2 class="h4 mb-0">2. Componentes y Ciclo de Aprendizaje</h2>
        </div>
        <div class="card-body">
            <div class="row mb-4">
                <div class="col-lg-6">
                    <h5 class="text-success">Elementos del Modelo</h5>
                    <ul class="list-unstyled">
                        <li><strong>Agente:</strong> Implementa la política y toma decisiones.</li>
                        <li><strong>Entorno:</strong> Mundo físico o simulado donde opera el agente.</li>
                        <li><strong>Estados (S):</strong> Información necesaria para decidir.</li>
                        <li><strong>Acciones (A):</strong> Decisiones discretas o continuas.</li>
                        <li><strong>Recompensas (R):</strong> Señal clave. Si se diseña mal, el agente puede aprender conductas indeseadas.</li>
                    </ul>
                </div>
                <div class="col-lg-6">
                    <h5 class="text-success">Principios del Ciclo</h5>
                    <p class="small">
                        <strong>Exploración vs Explotación:</strong> El agente usa estrategias como ε-greedy 
                        para equilibrar descubrir nuevas acciones (explorar) y usar las mejores conocidas (explotar).
                    </p>
                    <p class="small">
                        <strong>Descuento Temporal (γ):</strong> Regula la visión a futuro. Un γ cercano a 1 prioriza 
                        el largo plazo, mientras que uno bajo prioriza lo inmediato.
                    </p>
                </div>
            </div>
        </div>
    </div>

    <div class="card shadow-sm mb-5">
        <div class="card-header bg-info text-white">
            <h2 class="h4 mb-0">3. Algoritmos Principales</h2>
        </div>
        <div class="card-body">
            <ul class="nav nav-tabs" id="algoTab" role="tablist">
                <li class="nav-item" role="presentation">
                    <button class="nav-link active" id="qlearning-tab" data-bs-toggle="tab" data-bs-target="#qlearning" type="button" role="tab">Q-Learning</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="sarsa-tab" data-bs-toggle="tab" data-bs-target="#sarsa" type="button" role="tab">SARSA</button>
                </li>
                <li class="nav-item" role="presentation">
                    <button class="nav-link" id="dqn-tab" data-bs-toggle="tab" data-bs-target="#dqn" type="button" role="tab">Deep Q-Network</button>
                </li>
            </ul>
            <div class="tab-content p-4 border border-top-0 rounded-bottom" id="algoTabContent">
                
                <div class="tab-pane fade show active" id="qlearning" role="tabpanel">
                    <h5>Q-Learning (Off-Policy)</h5>
                    <p>Es un algoritmo <strong>off-policy</strong>: aprende la mejor política posible independientemente de la política que está usando para explorar.</p>
                    
                    <div class="alert alert-light text-dark text-center border shadow-sm">
                        <p class="mb-0 font-monospace fw-bold">
                            $$ Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a'} Q(s',a') - Q(s,a)) $$
                        </p>
                    </div>
                    
                    <small class="text-muted">Muy usado en entornos discretos.</small>
                </div>

                <div class="tab-pane fade" id="sarsa" role="tabpanel">
                    <h5>SARSA (On-Policy)</h5>
                    <p>Es <strong>on-policy</strong>: actualiza los valores usando la acción que realmente selecciona su política actual.</p>
                    <p>Es más "conservador" y seguro, ya que considera la exploración dentro del aprendizaje.</p>
                    
                    <div class="alert alert-light text-dark text-center border shadow-sm">
                        <p class="mb-0 font-monospace fw-bold">
                            $$ Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha(r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)) $$
                        </p>
                    </div>
                </div>

                <div class="tab-pane fade" id="dqn" role="tabpanel">
                    <h5>Deep Q-Network (DQN)</h5>
                    <p>
                        Cuando el espacio de estados es muy grande (ej. imágenes), no se pueden usar tablas. DQN usa <strong>redes neuronales profundas</strong> para aproximar Q(s,a).
                    </p>
                    <div class="mt-3">
                        <h6>Innovaciones Clave:</h6>
                        <ul>
                            <li><strong>Experience Replay:</strong> Almacena experiencias en memoria para entrenar en "lotes" aleatorios, rompiendo correlaciones.</li>
                            <li><strong>Target Network:</strong> Una red separada para calcular el valor objetivo, mejorando la estabilidad.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="card shadow-sm mb-5 border-warning">
        <div class="card-header bg-warning text-dark">
             <h3 class="h5 mb-0"><i class="fas fa-check-circle me-2"></i>Buenas Prácticas en RL</h3>
        </div>
        <div class="card-body">
            <div class="row">
                <div class="col-md-6">
                    <h6 class="text-primary">Estabilidad y Convergencia</h6>
                    <ul class="list-group list-group-flush mb-3">
                        <li class="list-group-item">
                            <strong>Experience Replay:</strong> Vital en DQN para romper la correlación entre datos consecutivos.
                        </li>
                        <li class="list-group-item">
                            <strong>Tasa de aprendizaje (α):</strong> Ajustarla cuidadosamente. Si es muy alta no converge; si es muy baja es demasiado lento.
                        </li>
                        <li class="list-group-item">
                            <strong>Factor de descuento (γ):</strong> Elegirlo según si el problema requiere visión a largo o corto plazo.
                        </li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <h6 class="text-primary">Exploración y Recompensas</h6>
                    <ul class="list-group list-group-flush mb-3">
                        <li class="list-group-item">
                            <strong>Decay de ε:</strong> Comenzar con alta exploración y reducirla gradualmente para evitar quedar en óptimos locales.
                        </li>
                        <li class="list-group-item">
                            <strong>Diseño de Recompensas:</strong> Evitar recompensas "engañosas" (ej. premiar velocidad si causa choques).
                        </li>
                        <li class="list-group-item">
                            <strong>Generalización:</strong> Usar regularización o aleatorizar el entorno para evitar la memorización.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <div class="card shadow-sm mb-5 border-danger">
        <div class="card-header bg-danger text-white">
            <h2 class="h4 mb-0">4. Conclusiones Clave</h2>
        </div>
        <div class="card-body">
            <p>
                El Aprendizaje por Refuerzo (RL) es un paradigma poderoso para la toma de decisiones secuenciales, caracterizado por la interacción dinámica entre un **Agente** y un **Entorno**. A diferencia del aprendizaje supervisado, el agente aprende a maximizar una recompensa acumulada a largo plazo a través de la experimentación (Exploración) y el uso del conocimiento actual (Explotación).
            </p>
            <ul class="list-group list-group-flush">
                <li class="list-group-item">
                    <strong>RL Clásico vs. Profundo:</strong> Algoritmos como **Q-Learning** (Off-Policy) y **SARSA** (On-Policy) son fundamentales y efectivos en espacios de estados discretos. Sin embargo, para problemas complejos con grandes espacios de estados (como la visión por computadora o entornos 3D), el **DQN** integra redes neuronales profundas para aproximar las funciones de valor, haciendo escalable el RL.
                </li>
                <li class="list-group-item">
                    <strong>Desafíos y Sintonización:</strong> La estabilidad del aprendizaje, la convergencia y la obtención de políticas óptimas dependen críticamente del balance entre exploración y explotación (controlado por **ε**), la visión a futuro (**γ**), y la calidad del **diseño de la recompensa**. Una implementación exitosa requiere una cuidadosa sintonización de hiperparámetros y el uso de técnicas estabilizadoras como el *Experience Replay*.
                </li>
            </ul>
        </div>
    </div>
    <div class="accordion" id="accordionReferences">
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingOne">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne">
                    <strong>Referencias Bibliográficas (APA 7)</strong>
                </button>
            </h2>
            <div id="collapseOne" class="accordion-collapse collapse">
                <div class="accordion-body">
                    <p class="small mb-1">Sutton, R. S., & Barto, A. G. (2018). <i>Reinforcement learning: An introduction</i> (2nd ed.). MIT Press.</p>
                    <p class="small mb-1">Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. <i>Nature, 518</i>(7540), 529-533.</p>
                    <p class="small mb-1">Arulkumaran, K., et al. (2017). Deep reinforcement learning: A brief survey. <i>IEEE Signal Processing Magazine</i>.</p>
                    <hr>
                    <p class="small mb-1">FlowHunt. (s.f.). <i>Glosario RL: Aprendizaje por Refuerzo</i>. Recuperado de flowhunt.io.</p>
                    <p class="small mb-1">Fundación Bankinter. (s.f.). <i>¿Qué es Q-Learning?</i> Recuperado de fundacionbankinter.org.</p>
                    <p class="small mb-1">FUOC. (s.f.). <i>Introducción al aprendizaje por refuerzo</i> (Documento PDF). Repositorio de Acceso Abierto UOC.</p>
                    <p class="small mb-1">Gavilán, I. G. R. (s.f.). <i>Notas sobre aprendizaje por refuerzo: SARSA y Q-Learning</i> [Blog]. Recuperado de Ignacio G.R. Gavilán.</p>
                    <p class="small mb-1">Notus.cl. (s.f.). <i>Qué es el Reinforcement Learning y cuáles son sus aplicaciones</i> [Artículo conceptual]. Recuperado de Notus.</p>
                    <p class="small mb-1">Tópicos Avanzados de IA – Aprendizaje por Refuerzo. (s.f.). Apuntes de curso universitario. Recuperado de rexemin.github.io.</p>
                    <p class="small mb-0">Universidad de Zaragoza / UNIZAR. (s.f.). <i>Tesis sobre modelos de Aprendizaje por Refuerzo</i>. Recuperado de Zagu.</p>
                </div>
            </div>
        </div>
    </div>

    <div class="d-flex justify-content-between my-4">
        <a href="{{ url_for('index') }}" class="btn btn-outline-primary">
            <i class="fas fa-home me-2"></i> Volver al inicio
        </a>
    </div>
</div>
{% endblock %}