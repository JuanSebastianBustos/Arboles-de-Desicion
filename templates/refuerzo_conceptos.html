{% extends "base.html" %}

{% block title %}Conceptos Básicos - Aprendizaje por Refuerzo{% endblock %}

{% block content %}
<div class="container mt-5 mb-5">
    
    <header class="text-center mb-5">
        <h1 class="display-4">Fundamentos de RL</h1>
        <p class="lead text-muted">Definiciones, Componentes y Ciclo de Aprendizaje</p>
    </header>

    <div class="card shadow-sm mb-5">
        <div class="card-header bg-primary text-white">
            <h2 class="h4 mb-0">1. Definición General y Diferencias</h2>
        </div>
        <div class="card-body">
            <p>
                El aprendizaje por refuerzo es una disciplina distinta dentro del aprendizaje automático. 
                A diferencia de otros paradigmas, aquí no hay un supervisor que indique "esta acción es la correcta". 
                El agente debe aprender a tomar decisiones para maximizar una señal (la recompensa).
            </p>
            
            <div class="alert alert-info mt-3">
                <i class="fas fa-clock me-2"></i><strong>Nota Importante:</strong> 
                Las consecuencias de las acciones pueden ser retrasadas en el tiempo; una acción puede tener un resultado mucho después de ser tomada.
            </div>

            <div class="row mt-4">
                <div class="col-md-6">
                    <div class="card h-100 border-secondary">
                        <div class="card-body">
                            <h5 class="card-title">Vs. Aprendizaje Supervisado</h5>
                            <p class="card-text">
                                No existe un "profesor" o etiquetas que indiquen la respuesta correcta. 
                                El agente aprende solo a través de la interacción y el error.
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-md-6">
                    <div class="card h-100 border-secondary">
                        <div class="card-body">
                            <h5 class="card-title">Vs. Aprendizaje No Supervisado</h5>
                            <p class="card-text">
                                No se limita a buscar patrones ocultos en los datos; el objetivo central es tomar decisiones activas para maximizar 
                                una recompensa acumulada.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="card shadow-sm mb-5">
        <div class="card-header bg-success text-white">
            <h2 class="h4 mb-0">2. Componentes del Modelo RL</h2>
        </div>
        <div class="card-body">
            <div class="row">
                <div class="col-md-6">
                    <ul class="list-group list-group-flush">
                        <li class="list-group-item">
                            <strong>Agente:</strong> Entidad (robot, software) que implementa una política y decide acciones según el estado.
                        </li>
                        <li class="list-group-item">
                            <strong>Entorno:</strong> El "mundo" (físico o simulado) donde opera el agente y que responde a sus acciones.
                        </li>
                        <li class="list-group-item">
                            <strong>Estados (S):</strong> Toda la información necesaria para tomar una decisión (ej. una imagen de pantalla en Atari).
                        </li>
                    </ul>
                </div>
                <div class="col-md-6">
                    <ul class="list-group list-group-flush">
                        <li class="list-group-item">
                            <strong>Acciones (A):</strong> Decisiones posibles. Pueden ser discretas (número limitado) o continuas (valores reales).
                        </li>
                        <li class="list-group-item">
                            <strong>Recompensas (R):</strong> Señal numérica de feedback. Un mal diseño aquí puede provocar comportamientos indeseados.
                        </li>
                        <li class="list-group-item">
                            <strong>Política (π):</strong> Función determinista o probabilística que asigna una acción a cada estado.
                        </li>
                    </ul>
                </div>
            </div>
            
            <div class="mt-4 p-4 bg-light rounded border border-success">
                <h5 class="text-success">Funciones de Valor</h5>
                <div class="row">
                    <div class="col-md-6">
                        <p><strong>V(s):</strong> Valor esperado del estado. Mide "qué tan bueno es estar en el estado s" bajo cierta política.</p>
                    </div>
                    <div class="col-md-6">
                        <p><strong>Q(s,a):</strong> Valor esperado de tomar la acción a en el estado s, y luego seguir la política.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="card shadow-sm mb-5">
        <div class="card-header bg-warning text-dark">
            <h2 class="h4 mb-0">3. Principios del Ciclo de Aprendizaje</h2>
        </div>
        <div class="card-body">
            <h5 class="text-primary">Exploración vs. Explotación</h5>
            <p>
                El dilema fundamental: el agente debe <strong>explorar</strong> para descubrir acciones nuevas (potencialmente mejores), 
                pero debe <strong>explotar</strong> lo aprendido para asegurar recompensas.
            </p>
            <ul>
                <li><strong>Estrategia ε-greedy:</strong> Con probabilidad ε (epsilon) se explora al azar; con probabilidad 1-ε se elige la mejor acción conocida.</li>
            </ul>
            
            <hr>
            
            <h5 class="text-primary">Retorno Acumulado y Descuento Temporal</h5>
            <p>
                El objetivo es maximizar el <strong>retorno total esperado</strong> (suma de recompensas futuras).
                El factor de descuento <strong>γ (gamma)</strong> regula la importancia del futuro:
            </p>
            <ul class="mb-0">
                <li>Si <strong>γ ≈ 1</strong>: El agente valora mucho las recompensas a largo plazo (planeación).</li>
                <li>Si <strong>γ es bajo</strong> (ej. 0.1): Se enfoca en recompensas inmediatas.</li>
            </ul>
        </div>
    </div>

    <div class="d-flex justify-content-between my-4">
        <a href="{{ url_for('index') }}" class="btn btn-outline-primary">
            <i class="fas fa-home me-2"></i> Volver al inicio
        </a>
    </div>
</div>
{% endblock %}